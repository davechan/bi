{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "import scraperwiki\n",
      "import cookielib\n",
      "import mechanize\n",
      "import lxml.html\n",
      "import re\n",
      "import cgi\n",
      "import requests\n",
      "import urllib, urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "\n",
      "myCalaisAPI_key = 'bj66f35p6h2tk5pswgdpxsuw' # your Calais API key.\n",
      "calaisREST_URL = 'http://api.opencalais.com/enlighten/rest/' # this is the older REST interface.\n",
      "\n",
      "calaisParams = '''\n",
      "<c:params xmlns:c=\"http://s.opencalais.com/1/pred/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
      "  <c:processingDirectives c:contentType=\"text/txt\"\n",
      "      c:enableMetadataType=\"SocialTags\"\n",
      "      c:outputFormat=\"Text/Simple\"/>\n",
      "  <c:userDirectives/>\n",
      "  <c:externalMetadata/>\n",
      "</c:params>\n",
      "'''\n",
      "\n",
      "\n",
      "br = mechanize.Browser()\n",
      "#br.set_handle_robots(False)   # no robots\n",
      "#br.set_handle_refresh(False)  # can sometimes hang without this\n",
      "br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "\n",
      "url = \"http://www.wisegiving.org.hk/en/donation/search/search.aspx\"\n",
      "\n",
      "br.open(url)\n",
      "\n",
      "br.select_form(\"aspnetForm\")         # works when form has a name\n",
      "\n",
      "cj = cookielib.LWPCookieJar()\n",
      "br.set_cookiejar(cj)\n",
      "\n",
      "#print br\n",
      "response1 = br.submit()\n",
      "\n",
      "the_page = response1.read()\n",
      "soup = BeautifulSoup(the_page)\n",
      "\n",
      "for resultorg in soup.findAll('div',{'class':'ResultText'}):\n",
      "    # Get link\n",
      "    orglink = resultorg('a')[0]['href']\n",
      "\n",
      "    # Get detail data from orglink\n",
      "    baseurl = \"http://www.wisegiving.org.hk\"\n",
      "    br2 = mechanize.Browser()\n",
      "    br2.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "    # get core services\n",
      "    urlservices = baseurl + str(orglink) + \"&page=report\"\n",
      "    br2.open(urlservices)\n",
      "    coreservices_page = br2.response().read()\n",
      "    soup1 = BeautifulSoup(coreservices_page)\n",
      "    coreservices = soup1.find('span',{'id':'ctl00_ContentPlaceHolder1_coreservice'})\n",
      "    stripcontent = unicode(re.sub('<.*?>','', str(coreservices.contents)), \"utf-8\")\n",
      "    print stripcontent\n",
      "    \n",
      "    dataToSend = urllib.urlencode({\n",
      "    'licenseID': myCalaisAPI_key,\n",
      "    'content': stripcontent,\n",
      "    'paramsXML': calaisParams\n",
      "    })\n",
      "\n",
      "    results = urllib2.urlopen(calaisREST_URL, dataToSend).read()\n",
      "    soup2 = BeautifulSoup(results)\n",
      "    for child in soup2.find_all('socialtag'):\n",
      "        if child.find_all('originalvalue'):\n",
      "            child.originalvalue.decompose()\n",
      "            if child['importance'] == \"1\":\n",
      "                print re.sub('-',' ',child.string)\n",
      "                data = {'OrgLink':str(orglink), 'Type':\"SocialTag\", 'Tag':re.sub('-',' ',child.string)}\n",
      "                scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "                \n",
      "    for child in soup2.find_all('industryterm'):\n",
      "        print re.sub('-',' ',child.string)\n",
      "        data = {'OrgLink':str(orglink), 'Type':\"IndustryTerm\",  'Tag':re.sub('-',' ',child.string)}\n",
      "        scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "\n",
      "    for child in soup2.find_all('topic'):\n",
      "        print re.sub('-',' ',child.string)\n",
      "        data = {'OrgLink':str(orglink), 'Type':\"Topic\",  'Tag':re.sub('-',' ',child.string)}\n",
      "        scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "\n",
      "# test out the pagination, loop from page 2 to 10-1, set max to highest page no + 1\n",
      "for i in range(2,60):\n",
      "#for i in range(2,2819):\n",
      "\n",
      "    br.select_form(\"aspnetForm\")         # works when form has a name\n",
      "    br.set_all_readonly(False)    # allow everything to be written to\n",
      "    br[\"__EVENTTARGET\"] = 'ctl00$ContentPlaceHolder1$GridView1'\n",
      "    br[\"__EVENTARGUMENT\"] = 'Page$' + str(i)\n",
      "\n",
      "    imagebutton1 = br.form.find_control(\"ctl00$ContentPlaceHolder1$ImageButton1\")\n",
      "    imagebutton2 = br.form.find_control(\"ctl00$ContentPlaceHolder1$ImageButton2\")\n",
      "    br.form.controls.remove(imagebutton1)\n",
      "    br.form.controls.remove(imagebutton2)\n",
      "\n",
      "    response2 = br.submit()\n",
      "    \n",
      "    the_page = response2.read()\n",
      "    soup = BeautifulSoup(the_page)\n",
      "    if i in range(13,60):\n",
      "        for resultorg in soup.findAll('div',{'class':'ResultText'}):\n",
      "            if resultorg('a')[0].has_key('href'):\n",
      "                orglink = resultorg('a')[0]['href']\n",
      "                # Pull details if orglink belongs to WiseGiving\n",
      "                if \"ngodetails.aspx\" in str(orglink):\n",
      "                \n",
      "                    # Get detail data from orglink\n",
      "                    baseurl = \"http://www.wisegiving.org.hk\"\n",
      "                    br2 = mechanize.Browser()\n",
      "                    br2.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "                    # get core services\n",
      "                    urlservices = baseurl + str(orglink) + \"&page=report\"\n",
      "                    br2.open(urlservices)\n",
      "                    coreservices_page = br2.response().read()\n",
      "                    soup1 = BeautifulSoup(coreservices_page)\n",
      "                    coreservices = soup1.find('span',{'id':'ctl00_ContentPlaceHolder1_coreservice'})\n",
      "                    print coreservices\n",
      "                    print coreservices.contents\n",
      "                    stripcontent = unicode(re.sub('<.*?>','', str(coreservices.contents)), \"utf-8\")\n",
      "                    print stripcontent\n",
      "                    \n",
      "                    myCalaisAPI_key = 'bj66f35p6h2tk5pswgdpxsuw' # your Calais API key.\n",
      "                    calaisREST_URL = 'http://api.opencalais.com/enlighten/rest/' # this is the older REST interface.\n",
      "                    \n",
      "                    calaisParams = '''\n",
      "                    <c:params xmlns:c=\"http://s.opencalais.com/1/pred/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
      "                      <c:processingDirectives c:contentType=\"text/txt\"\n",
      "                          c:enableMetadataType=\"SocialTags\"\n",
      "                          c:outputFormat=\"Text/Simple\"/>\n",
      "                      <c:userDirectives/>\n",
      "                      <c:externalMetadata/>\n",
      "                    </c:params>\n",
      "                    '''\n",
      "\n",
      "                    dataToSend = urllib.urlencode({\n",
      "                    'licenseID': myCalaisAPI_key,\n",
      "                    'content': stripcontent,\n",
      "                    'paramsXML': calaisParams\n",
      "                    })\n",
      "\n",
      "                    results = urllib2.urlopen(calaisREST_URL, dataToSend).read()\n",
      "                    soup2 = BeautifulSoup(results)\n",
      "                    for child in soup2.find_all('socialtag'):\n",
      "                        if child.find_all('originalvalue'):\n",
      "                            child.originalvalue.decompose()\n",
      "                            if child['importance'] == \"1\":\n",
      "                                print re.sub('-',' ',child.string)\n",
      "                                data = {'OrgLink':str(orglink), 'Type':\"SocialTag\", 'Tag':re.sub('-',' ',child.string)}\n",
      "                                scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "                                \n",
      "                    for child in soup2.find_all('industryterm'):\n",
      "                        print re.sub('-',' ',child.string)\n",
      "                        data = {'OrgLink':str(orglink), 'Type':\"IndustryTerm\",  'Tag':re.sub('-',' ',child.string)}\n",
      "                        scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "\n",
      "                    for child in soup2.find_all('topic'):\n",
      "                        print re.sub('-',' ',child.string)\n",
      "                        data = {'OrgLink':str(orglink), 'Type':\"Topic\",  'Tag':re.sub('-',' ',child.string)}\n",
      "                        scraperwiki.sql.save(unique_keys=[], data=data, table_name=\"tags\")\n",
      "                     \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'charmap' codec can't encode character u'\\u2013' in position 44: character maps to <undefined>",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-14-6a9515c27884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m                             \u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginalvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'importance'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                 \u001b[1;32mprint\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'OrgLink'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morglink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Type'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"SocialTag\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Tag'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                                 \u001b[0mscraperwiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tags\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Dave\\Anaconda\\lib\\encodings\\cp437.pyc\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, errors)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character u'\\u2013' in position 44: character maps to <undefined>"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}